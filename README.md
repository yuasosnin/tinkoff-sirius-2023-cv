# Введение

![image](/plots/alien.png)
```
a humanoid deep sea fish toursit in sunglsses and with a suitcase
```

Мои инопланетяне живут на далекой от их звезды пленете, на которую почти не попадает евтественный свет. Прибывая на землю, они в принципе подготовлены к земному солнечному свету, но фотографии с естественным освещением могут им подсознательно не понравиться. В остальном их предпочтения совпадают с обычными гуманоидными предпочтениями.
Таким образом, чтобы отели понравились гостям, нужно будет изменить освещение фотографий. 

# Датасет

Датасет фото отелей в этой задаче - Hotels-50K. Он изначально предназначен для задачи scene/place recognition - то есть определения отеля или сети отелей, в которых сделано фото. Это может быть как классификация сети или конкретного отеля, так и retireval топ-k кандидатов. Решение этой задачи помогло бы правоохранительным органам в расследовании торговли людьми: многие фото жертв в интернете сделаны в отелях. Датасет состоит из двух источников: промо-фотографий с сайтов отелей и фото пользователей приложения TraffickCam, которые добровольно фотографируют свои номера чтобы помочь общему делу. Большинство фото - рекламные, они в среднем довольно низкого разрешения. Всего в датасете 50к отелей, 93 сети отелей, и больше миллиона фотографий.

Более подробно про EDA и Image Capitoning - в ноутбуке.

# Image Capitoning

Image Captioning - это задача преобразования картинки в текстовое описание. Image Captioning модель должна из картинки получать описание того, что на ней изображено.

CLIP - self-supervised модель сопоставления картинок и описаний, известна тем, что используется в Stable Diffusion для text-to-image guidance. И текст, и картинка отдельными энкодерами преобразовываются в эмбеддинги, и обучается так, чтобы сопоставлять в соответствующие картинки и тексты в батче.

Одним из подходов для подбора тегов к нашим изображениям отелей было бы составление какого-то закрытого списка тегов, и ранжирование логитов CLIPа относительно этих тегов. Недостаток такого подхода в том, что, во-первых, нужен этот список, а во-вторых, не понятно, по какому принципу брать теги - это либо топ-k наиболее похожих, либо по какому-то порогу, и так и так гиперпараметр, который пришлось бы подбирать. Другой подход - использовать полноценную генеративную image captioning модель.

Самая популярная такая модель - BLIP. Грубо говоря, это ViT в виде энкодера и GPT и виде декодера в ванильной архитектуре трансформера. Кроме того, у BLIP еще несколько разных тренировочных задач (непосредственно captioning, классификация того, что описание подходит, и вычисление сходства описания), на которые он обучается одновременно, и специфическая процедура обучения, с отсеиванием и переделыванием неподходящих описаний из изначального датасета. Нас тут интересует то, что BLIP, это, по сути, просто транфсормер, который "переводит" картинку в текстовое описание. 

Можно заметить, что CLIP Interrogator из AUTOMATIC1111 - это, по сути, BLIP для генерации описания, плюс CLIP на заранее определенный набор стилей и авторов.

![image](/plots/captions.png)
```
tables and chairs in a courtyard with a wall and trees
there is a living room with a couch , chair , television and a dresser
a ##raf ##ed living room with a couch , table , and a kitchen ##ette
a close up of a framed photograph of a woman in a hat
there is a large living room with a couch , chair , table and television
there is a living room with a couch , chair , and coffee table
purple living room with a couch and a table in front of a window
there is a living room with a couch , table , and television
```

Более подробные результаты - в ноутбуке.

# Генеративное редактирование

Диффузионные модели умеют генерировать изображения в соотсетвтвии и текстовым промптом. В последнее время вокруг одной из больших моделей, Stable Diffusion, развелось большое сообщество, активно развивающее разные методы и приложения.
В этом задании нас интересует не просто text-to-image генерация, а именно редактирование уже заданной фотографии. Есть несколько способов это осуществить.

Для начала, нужно заметить, что стандартный чекпоинт SD-1.5 плохо справляется с контрастными изображениями, это известная его особенность. Чтобы успешно генерировать ночь, я буду использовать LoRA чекпоинт [Theovercomer8's Contrast Fix](https://civitai.com/models/8765/theovercomer8s-contrast-fix-sd15sd21-768). 
LoRA - это способ дообучения больших моделей, пришедший из NLP. Идея в том, что можно обучать не сами веса, а дельту весов, и оказывается, что в реальности эта дельта может быть очень низкого ранга, что сильно уменьшает и время обучения, и компьют, и размер дообученного чекпоинта.

Итак, есть несколько способов использовать Stable Diffusion для редактирования конкретных картинок. Во-первых, просто вкладка "image-to-image" в WebUI. Как я понял, ее имплементация - это идея из статьи про SDEdit (вообще сообщество не всегда документирует что оно делает, и не у всех идей даже есть статьи). 
Смысл там в том, чтобы взять какую-то картинку - закрасить область нужным цветом, либо просто скетч с цветами, либо просто оригинальная картинку - и прогнать диффузионную модель не с нулевого шума, а с не до конца зашумленной этой картинкой. Таким образом на выходе должно получиться реалистичное и соответствующее условию изображение.

![image](/plots/1.jpg)

Это картинка из TrafficCam, так как промо-картинки - низкого качества.

![image](/plots/i2i-1.png)
```
a hotel room at night, deep shadows, good quality
```

Как можно заметить, этот метод не работает. Кроме того, что освещение не меняется, также не удается сохранить детали, типа цвета комода, формы ручке, и т.д.

Второй способ - ControlNet или T2I Adapters - разные имплементации одной идеи. Можно взять, и обучить дополнительную модель на разные домены условий, активации которой прибавляются к замороженному Unet'у SD так, чтобы получалась нужная картинка. Этими условиями могут быть, например, карты сегментации, распознанные специальным алгоритмом края, глубина, поза, и т.д.

![image](/plots/canny-1.png)

Canny edge detection алгоритм как один из инпутов ControlNet

![image](/plots/depth-1.png)

Depth estimation

![image](/plots/controlnet-1.png)
```
a hotel room at night, deep shadows, good quality
```

Этот результат уже лучше, фото уже того же отеля, и освещение нужное, но плохо получается сохранять цвета и мелкие детали.

Наконец, метод, который дал лучший результат - InstructPix2Pix. В отличие от предыдущих методов, этот требует своего чекпоинта, т.е. обучения полноценной модели с нуля. Авторы использовали GPT-3 и спецаильный метод генерации похожих, отличающихся в деталях, картинок, чтобы создать синтетический датасет картинок, измененных "по инструкции". Итоговая модель генерирует изображения с двумя условиями, не только на промпт, но и на версию картинки "до" изменения, которое в промпте. Таким образом, эта модель умеет делать изменения типа "make this in winter", или "replace mountains with the city".

![image](/plots/instruct-1.png)
```
make this at night, deep shadows, good quality
```

Соответственно, промпт "make this at night" уже сразу делает нужное изменение. Есть и минусы: у меня так и не получилось закрыть шторы или выключить лампы. Ну ничего, инопланетянам и так норм.

# Список литературы

- [AUTOMATIC1111 WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)
- [Hotels-50K: A Global Hotel Recognition Dataset](https://arxiv.org/abs/1901.11397)
- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
- [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086)
- [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations](https://arxiv.org/abs/2108.01073)
- [T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.08453)
- [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543)
- [InstructPix2Pix: Learning to Follow Image Editing Instructions](https://arxiv.org/abs/2211.09800)
